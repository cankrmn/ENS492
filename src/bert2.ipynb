{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/cankrmn/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/cankrmn/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import contractions\n",
    "import inflect\n",
    "# from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "import re, string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./reduced_dataset[0,1000].csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test= train_test_split(df, test_size=0.15, random_state=42)\n",
    "train, validation= train_test_split(train, test_size=0.20, random_state=42)\n",
    "\n",
    "train.to_csv('train_data.csv', index=False)\n",
    "validation.to_csv('validation_data.csv', index=False)\n",
    "test.to_csv('test_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('csv', data_files={'train': ['train_data.csv'], 'test': 'test_data.csv', 'validation' : 'validation_data.csv'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': 284,\n",
       " 'raw_text': 'Operation Diànxùn: Cyberespionage Campaign Targeting Telecommunication Companies\\nhttps://www.mcafee.com/blogs/other-blogs/mcafee-labs/operation-dianxun-cyberespionage-campaign-targeting-telecommunication-companies/',\n",
       " 'fraud': 0,\n",
       " 'hacker groups': 0,\n",
       " 'government': 0,\n",
       " 'corporation': 0,\n",
       " 'unrelated': 0,\n",
       " 'darknet': 0,\n",
       " 'cyber defense': 0,\n",
       " 'hacking': 0,\n",
       " 'security concepts': 0,\n",
       " 'security products': 0,\n",
       " 'network security': 0,\n",
       " 'cyberwar': 0,\n",
       " 'geopolitical': 1,\n",
       " 'data breach': 0,\n",
       " 'vulnerability': 0,\n",
       " 'platform': 0,\n",
       " 'cyber attack': 1}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = dataset['train'][1]\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_remover(data): # remove any url in text\n",
    "  return re.sub(r'https?\\S+','',data)\n",
    "\n",
    "def web_associated(data):\n",
    "  text = url_remover(text)\n",
    "  return text\n",
    "\n",
    "# -------------------------------\n",
    "\n",
    "def remove_round_brackets(data): # remove anything between two round brackets\n",
    "   return re.sub('\\(.*?\\)','',data)\n",
    "\n",
    "punctList = string.punctuation + '“”' \n",
    "def remove_punc(data): # remove any punctuation\n",
    "  trans = str.maketrans('','', punctList)\n",
    "  return data.translate(trans)\n",
    "\n",
    "def white_space(data): # remove any double or more space\n",
    "  return ' '.join(data.split())\n",
    "\n",
    "def complete_noise(data):\n",
    "  new_data = remove_round_brackets(data)\n",
    "  new_data = remove_punc(new_data)\n",
    "  new_data = white_space(new_data)\n",
    "  return new_data\n",
    "\n",
    "# -------------------------------\n",
    "def text_lower(data): # make every letter lowercase\n",
    "  return data.lower()\n",
    "\n",
    "def contraction_replace(data): # fix contractions (e.g. won't => will not)\n",
    "  return contractions.fix(data)\n",
    "\n",
    "def number_to_text(data): # write numbers as text and return (...12... => ...twelve...)\n",
    "  temp_str = data.split()\n",
    "  string = \"\"\n",
    "  for i in temp_str:\n",
    "    if i.isdigit(): # if the word is digit, converted to \n",
    "      temp = inflect.engine().number_to_words(i)\n",
    "      string += temp + \" \"\n",
    "    else:\n",
    "      string += i + \" \"\n",
    "  return string.strip()\n",
    "\n",
    "def normalization(data):\n",
    "  text = text_lower(data)\n",
    "  text = number_to_text(text)\n",
    "  text = contraction_replace(text)\n",
    "  tokens = nltk.word_tokenize(text)\n",
    "  return tokens\n",
    "\n",
    "# -------------------------------\n",
    "\n",
    "def stopword(data): # remove stopwords\n",
    "  clean = []\n",
    "  for i in data:\n",
    "    if i not in stopwords.words('english'):\n",
    "      clean.append(i)\n",
    "  return clean\n",
    "\n",
    "def stemming(data): # stem the text\n",
    "  stemmer = LancasterStemmer()\n",
    "  stemmed = []\n",
    "  for i in data:\n",
    "    stem = stemmer.stem(i)\n",
    "    stemmed.append(stem)\n",
    "  return stemmed\n",
    "\n",
    "def lemmatization(data): # lemmatize the text\n",
    "  lemma = WordNetLemmatizer()\n",
    "  lemmas = []\n",
    "  for i in data:\n",
    "    lem = lemma.lemmatize(i, pos='v')\n",
    "    lemmas.append(lem)\n",
    "  return lemmas  \n",
    "\n",
    "def final_process(data):\n",
    "  stopwords_remove = stopword(data)\n",
    "  stemmed = stemming(stopwords_remove)\n",
    "  lemm = lemmatization(stopwords_remove)\n",
    "  return stemmed, lemm\n",
    "\n",
    "# -------------------------------\n",
    "\n",
    "def preprocess(data): # run all preprocessing functions\n",
    "  txt = url_remover(data)\n",
    "  txt = complete_noise(txt)\n",
    "  txt = normalization(txt)\n",
    "  stemmed, lemm = final_process(txt)\n",
    "  return stemmed, lemm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sec', 'two', 'hundr', 'thirteen', 'dat', 'frenzy', 'remot', 'workforcepodcast', 'stock', 'liqu', 'cabinet', 'tak', 'shot', 'whenev', 'hear', 'gitlab', 'staff', 'sec', 'research', 'mark', 'loveless', 'say', 'zero', 'trust'] \n",
      "\n",
      " ['secure', 'two', 'hundred', 'thirteen', 'data', 'frenzied', 'remote', 'workforcepodcast', 'stock', 'liquor', 'cabinet', 'take', 'shoot', 'whenever', 'hear', 'gitlab', 'staff', 'security', 'researcher', 'mark', 'loveless', 'say', 'zero', 'trust']\n"
     ]
    }
   ],
   "source": [
    "txt = '\"Securing 213 Data With a Frenzied Remote Workforce-Podcast https://threatpost.com/securing-data-frenzied-remote-workforce-podcast/178742/ Stock the liquor cabinet and take a shot whenever you hear GitLab Staff Security Researcher Mark Loveless say “Zero Trust.”\"'\n",
    "\n",
    "stemmed, lemm = preprocess(txt)\n",
    "\n",
    "print(stemmed, \"\\n\\n\", lemm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['fraud', 'hacker groups', 'government', 'corporation',\n",
    "       'unrelated', 'darknet', 'cyber defense', 'hacking', 'security concepts',\n",
    "       'security products', 'network security', 'cyberwar', 'geopolitical',\n",
    "       'data breach', 'vulnerability', 'platform', 'cyber attack']\n",
    "\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a4a161252db59fab3a59ea67bc9ddff194f6949cc2b93320e6d42b4ff9dbbca4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
