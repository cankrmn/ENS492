{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# General (Start here)\n"
      ],
      "metadata": {
        "id": "vaJ3tKZM6xll"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports - BeatifulSoup, Selenium, ... "
      ],
      "metadata": {
        "id": "sZZZKmP37UI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium\n",
        "!apt-get update \n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "import sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "from selenium import webdriver\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "#from selenium.common.exceptions import NoSuchElementException"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQewBGtI7X5d",
        "outputId": "45f484fd-039a-4c70-e4f8-a67e8bb0c7f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.7.2-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trio-websocket~=0.9\n",
            "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
            "Collecting trio~=0.17\n",
            "  Downloading trio-0.22.0-py3-none-any.whl (384 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.9/384.9 KB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.8/dist-packages (from selenium) (2022.12.7)\n",
            "Collecting urllib3[socks]~=1.26\n",
            "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.8/dist-packages (from trio~=0.17->selenium) (22.2.0)\n",
            "Collecting outcome\n",
            "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.8/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Collecting sniffio\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Collecting async-generator>=1.9\n",
            "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.8/dist-packages (from trio~=0.17->selenium) (2.10)\n",
            "Collecting exceptiongroup>=1.0.0rc9\n",
            "  Downloading exceptiongroup-1.1.0-py3-none-any.whl (14 kB)\n",
            "Collecting wsproto>=0.14\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
            "Collecting h11<1,>=0.9.0\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 KB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: urllib3, sniffio, outcome, h11, exceptiongroup, async-generator, wsproto, trio, trio-websocket, selenium\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed async-generator-1.10 exceptiongroup-1.1.0 h11-0.14.0 outcome-1.2.0 selenium-4.7.2 sniffio-1.3.0 trio-0.22.0 trio-websocket-0.9.2 urllib3-1.26.14 wsproto-1.2.0\n",
            "Hit:1 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
            "Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  Release\n",
            "Get:7 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
            "Get:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease [18.1 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,284 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [2,909 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [2,003 kB]\n",
            "Get:17 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal/main Sources [2,377 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2,436 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [982 kB]\n",
            "Get:20 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal/main amd64 Packages [1,126 kB]\n",
            "Fetched 13.5 MB in 6s (2,328 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  apparmor chromium-browser liblzo2-2 snapd squashfs-tools\n",
            "Suggested packages:\n",
            "  apparmor-profiles-extra apparmor-utils\n",
            "The following NEW packages will be installed:\n",
            "  apparmor chromium-browser chromium-chromedriver liblzo2-2 snapd\n",
            "  squashfs-tools\n",
            "0 upgraded, 6 newly installed, 0 to remove and 29 not upgraded.\n",
            "Need to get 38.4 MB of archives.\n",
            "After this operation, 173 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 apparmor amd64 2.13.3-7ubuntu5.1 [494 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal/main amd64 liblzo2-2 amd64 2.10-2 [50.8 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 squashfs-tools amd64 1:4.4-1ubuntu0.3 [117 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 snapd amd64 2.57.5+20.04ubuntu0.1 [37.6 MB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 chromium-browser amd64 1:85.0.4183.83-0ubuntu0.20.04.2 [48.3 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 chromium-chromedriver amd64 1:85.0.4183.83-0ubuntu0.20.04.2 [2,496 B]\n",
            "Fetched 38.4 MB in 3s (15.0 MB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package apparmor.\n",
            "(Reading database ... 129504 files and directories currently installed.)\n",
            "Preparing to unpack .../apparmor_2.13.3-7ubuntu5.1_amd64.deb ...\n",
            "Unpacking apparmor (2.13.3-7ubuntu5.1) ...\n",
            "Selecting previously unselected package liblzo2-2:amd64.\n",
            "Preparing to unpack .../liblzo2-2_2.10-2_amd64.deb ...\n",
            "Unpacking liblzo2-2:amd64 (2.10-2) ...\n",
            "Selecting previously unselected package squashfs-tools.\n",
            "Preparing to unpack .../squashfs-tools_1%3a4.4-1ubuntu0.3_amd64.deb ...\n",
            "Unpacking squashfs-tools (1:4.4-1ubuntu0.3) ...\n",
            "Selecting previously unselected package snapd.\n",
            "Preparing to unpack .../snapd_2.57.5+20.04ubuntu0.1_amd64.deb ...\n",
            "Unpacking snapd (2.57.5+20.04ubuntu0.1) ...\n",
            "Setting up apparmor (2.13.3-7ubuntu5.1) ...\n",
            "Created symlink /etc/systemd/system/sysinit.target.wants/apparmor.service → /lib/systemd/system/apparmor.service.\n",
            "Setting up liblzo2-2:amd64 (2.10-2) ...\n",
            "Setting up squashfs-tools (1:4.4-1ubuntu0.3) ...\n",
            "Setting up snapd (2.57.5+20.04ubuntu0.1) ...\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.aa-prompt-listener.service → /lib/systemd/system/snapd.aa-prompt-listener.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.apparmor.service → /lib/systemd/system/snapd.apparmor.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.autoimport.service → /lib/systemd/system/snapd.autoimport.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.core-fixup.service → /lib/systemd/system/snapd.core-fixup.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.recovery-chooser-trigger.service → /lib/systemd/system/snapd.recovery-chooser-trigger.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Created symlink /etc/systemd/system/cloud-final.service.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.service → /lib/systemd/system/snapd.service.\n",
            "Created symlink /etc/systemd/system/timers.target.wants/snapd.snap-repair.timer → /lib/systemd/system/snapd.snap-repair.timer.\n",
            "Created symlink /etc/systemd/system/sockets.target.wants/snapd.socket → /lib/systemd/system/snapd.socket.\n",
            "Created symlink /etc/systemd/system/final.target.wants/snapd.system-shutdown.service → /lib/systemd/system/snapd.system-shutdown.service.\n",
            "Selecting previously unselected package chromium-browser.\n",
            "(Reading database ... 129794 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-browser_1%3a85.0.4183.83-0ubuntu0.20.04.2_amd64.deb ...\n",
            "=> Installing the chromium snap\n",
            "==> Checking connectivity with the snap store\n",
            "===> System doesn't have a working snapd, skipping\n",
            "Unpacking chromium-browser (1:85.0.4183.83-0ubuntu0.20.04.2) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_1%3a85.0.4183.83-0ubuntu0.20.04.2_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (1:85.0.4183.83-0ubuntu0.20.04.2) ...\n",
            "Setting up chromium-browser (1:85.0.4183.83-0ubuntu0.20.04.2) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (1:85.0.4183.83-0ubuntu0.20.04.2) ...\n",
            "Processing triggers for mime-support (3.64ubuntu1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n",
            "Processing triggers for systemd (245.4-4ubuntu3.19) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Processing triggers for dbus (1.12.16-2ubuntu2.3) ...\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import string\n",
        "import re\n",
        "import json\n",
        "import time"
      ],
      "metadata": {
        "id": "F_RPYlRVE0k2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tags List"
      ],
      "metadata": {
        "id": "TpXiBr-kC9ZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_tags = [\"fraud\", \"hacker groups\", \"government\", \"corporation\", \"darknet\", \"incident response\", \"cyber intelligence\", \"antivirus\", \n",
        "            \"forensics\", \"pen testing\", \"firewall\", \"cyber defense\", \"hacking\", \"white hat\", \"black hat\", \"stenography\", \"cryptography\",\n",
        "            \"cloud security\", \"botnet\", \"IoT\", \"DDOS\", \"network security\",\"cyberwar\", \"usa\", \"russia\", \n",
        "            \"ukraine\", \"cyberterrorism\", \"data breach\", \"security breach\", \"vulnerability\", \"cve\", \"XSS\", \"patch\", \"mobile\", \"OS\", \"platform\",\n",
        "            \"cyber attack\", \"malware\", \"virus\", \"phishing\", \"adware\", \"rootkit\", \"backdoor\", \"keylog\", \"trojan\", \n",
        "            \"ransomware\", \"spyware\"]"
      ],
      "metadata": {
        "id": "GZFzi3LODAgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gizmodo (Crawler, Search)"
      ],
      "metadata": {
        "id": "INeQ1hrV8d4f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crawler"
      ],
      "metadata": {
        "id": "oSLenX1H8jek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import string\n",
        "import re\n",
        "\n",
        "header = {\n",
        "  \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.82 Safari/537.36\"\n",
        "}\n",
        "\n",
        "url1 = \"https://gizmodo.com/china-mysterious-spaceplane-ejects-unknown-object-1849733129\"\n",
        "\n",
        "def scrapeGizmodo(url):\n",
        "  html_text = requests.get(url, headers=header).text\n",
        "  soup = BeautifulSoup(html_text, \"lxml\")\n",
        "  dic = {}\n",
        "  textContainer = soup.find(\"div\", class_=\"xs32fe-0 iOFxrO js_post-content\")\n",
        "  if (not textContainer == None):\n",
        "    pArray = textContainer.find_all(\"p\")\n",
        "    #print(pArray)\n",
        "    raw_text = getText(pArray, ['<span>(.*?)\">', '<\\/a><\\/span>', '<p.*?\">', '<\\/p>,', '<.*?>'])\n",
        "    dic[\"raw text\"] = raw_text\n",
        "  return dic\n",
        "\n",
        "def getText(text, regexList):\n",
        "  newText = text\n",
        "  for regex in regexList:\n",
        "    newText = re.sub(regex, '', str(newText))\n",
        "  return newText"
      ],
      "metadata": {
        "id": "jh8uzy308lft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scrapeGizmodo(url1)[\"raw text\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "aRVo30eQCU2A",
        "outputId": "60fd6b12-e312-4a1f-e981-18c51d58986e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[The saga of China’s spaceplane continues as the experimental vehicle just released a mystery object that’s now closely trailing behind in low Earth orbit.   On Monday, the United States Space Force’s 18th Space Defense Squadron tracked an object in a similar orbit to the spaceplane, SpaceNews first reported. The object appeared to be very close to the spaceplane. So close, in fact, that the Space Force unit had to make sure it was a separate object before it was entered into the database as such. The object may have been ejected earlier from the spaceplane, perhaps between October 24 to 30, but it got added to the database on October 31, according to a tweet by Robert Christy from Orbital Focus. Jonathan McDowell, an astronomer at the Harvard-Smithsonian Center for Astrophysics, suggested that the released object may be a service module, which houses support systems for its accompanying spacecraft. The object’s recent ejection may be “possibly indicating an upcoming deorbit burn,” McDowell tweeted on Monday. The spaceplane’s orbit forces it to fly over a possible landing site  at Lop Nur base in Xinjiang (where China’s first version of spaceplane landed on September 6, 2020) every three days, according to SpaceNews. The spaceplane may have released other objects shortly after launching this past August. Tracking data from the Space Defense Squadron noted at least one or two objects that may be inspector satellites tracking the spacecraft itself, according to Space News. If you haven’t been following the orbital journey of the experimental vehicle, here’s a quick recap.  Reserve the next gen Samsung deviceOn August 4, China sneakily launched a reusable spacecraft as a classified payload onboard a Long March 2F carrier rocket from the Jiuquan Satellite Launch Center, state media reported at the time. This marked China’s second attempt to test its spaceplane, and it’s now breaking much higher ground. The first test saw the spaceplane stay in orbit for about two days and release a small payload before landing back on Earth. The current spaceplane keeps going strong with three months and counting in orbit, but as McDowell suggests, the mission might be coming to a close.  The experimental vehicle recently raised its low Earth orbit from about 215  by 369 miles (346 kilometers by 593 kilometers) above the surface to  a near-circular 371 by 378 miles (597 by 608 km). China’s first spaceplane flew at an altitude of about 206 by 216 miles with a similar inclination (331  by 347 km). China has shared very little information about its precious spaceplane, simply stating that it would stay in orbit for a “period of time.” The experimental vehicle is a product of the China Academy of Launch Vehicle Technology, a state-owned manufacturer that makes both civilian and military space launch vehicles. Similarly, the U.S. Space Force has its own spaceplane; the Boeing X-37 launched in May 2020 for its sixth test flight and has been in orbit ever since. As the name suggests, spaceplanes are airplane-spacecraft hybrids operating like regular aircraft in Earth’s atmosphere and orbiting spacecraft once in space. The reusable spacecraft can perform horizontal landings on Earth, similar to a plane landing. If proven successful, spaceplanes could become a useful reusable spacecraft to meet a growing demand from satellites and other missions needing rides to space. More: China Launched the Final Module for Its Space Station]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Search"
      ],
      "metadata": {
        "id": "Ew-Bj1v6_PM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import string\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "\n",
        "driver = webdriver.Chrome('chromedriver',options=chrome_options)\n",
        "search_gizmodo_dic = collections.defaultdict(list)\n",
        "\n",
        "def end_of_page_check(soup, min_year, news_content):\n",
        "  try:\n",
        "    if(soup.find(\"a\", {\"class\": \"peggds-2 ixaYpK next-button\"})):\n",
        "      return False\n",
        "    else:\n",
        "      date_year = re.search('datetime=\"(\\d*?)-', str(news_content))\n",
        "      if date_year is not None:\n",
        "        if(int(date_year.group(1)) > min_year):\n",
        "          return False\n",
        "      return True\n",
        "  except:\n",
        "    print(\"Error in eop check\")\n",
        "    return True\n",
        "\n",
        "def search_gizmodo(keyword, min_year, max_page):#usage: search_gizmodo(\"hacking\", 2019, 4)\n",
        "  end_of_page = False;\n",
        "  page_count = 1\n",
        "  dic_list = []\n",
        "\n",
        "  while(not end_of_page):\n",
        "    #goto url\n",
        "    print(page_count)\n",
        "    keyword = keyword.replace(' ', '%20')\n",
        "    if(page_count == 1):\n",
        "      url = \"https://gizmodo.com/tag/\" + keyword\n",
        "      #url = \"https://gizmodo.com/search?blogId=4&q=\" + china%20hacked\"\n",
        "    else:\n",
        "      url = \"https://gizmodo.com/tag/\" + keyword + \"?startIndex=\" + str(page_count * 20)\n",
        "    driver.get(url)\n",
        "    driver.maximize_window()\n",
        "    time.sleep(1)\n",
        "\n",
        "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
        "    news_content = soup.find_all(\"div\", {\"class\": \"cw4lnv-5 aoiLP\"}) #get all news in page\n",
        "\n",
        "    if(max_page <= page_count):\n",
        "      end_of_page = True\n",
        "    if(len(news_content) != 0):\n",
        "      if(end_of_page_check(soup, min_year, news_content)):#is this the last page to iterate\n",
        "        end_of_page = True\n",
        "\n",
        "      for news in news_content:#get urls from news\n",
        "        dic = {}\n",
        "        news_url = re.search('\"(https.*?)\"', str(news))\n",
        "        #print(news_url.group(1))\n",
        "        if(news_url):\n",
        "          if keyword not in search_gizmodo_dic[news_url.group(1)]:\n",
        "            search_gizmodo_dic[news_url.group(1)].append(keyword)\n",
        "    page_count += 1\n",
        "      #print(\"page:\" + str(page_count))\n",
        "\n",
        "  #print(\"End, page count: \" + str(page_count))"
      ],
      "metadata": {
        "id": "V845RXh8_THN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_gizmodo(\"hacking\", 2017, 5)"
      ],
      "metadata": {
        "id": "zSxYB8DY8hlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNET (Crawler)"
      ],
      "metadata": {
        "id": "LvyWiGaxATSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import string\n",
        "import re\n",
        "\n",
        "header = {\n",
        "  \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.82 Safari/537.36\"\n",
        "}\n",
        "\n",
        "url1 = \"https://www.cnet.com/tech/home-entertainment/apple-tv-4k-2022-review-in-progress-cheaper-price-new-chip-only-go-so-far/\"\n",
        "\n",
        "def scrapeCnet(url):\n",
        "  html_text = requests.get(url, headers=header).text\n",
        "  soup = BeautifulSoup(html_text, \"lxml\")\n",
        "  dic = {}\n",
        "  textContainer = soup.find(\"div\", class_=\"col-6 article-main-body row\")\n",
        "  pArray = textContainer.find_all(\"p\")\n",
        "  raw_text = getText(pArray, ['<.*?>'])\n",
        "  dic[\"raw text\"] = raw_text\n",
        "  #print(dic[\"raw text\"])\n",
        "  return dic\n",
        "\n",
        "def getText(text, regexList):\n",
        "  newText = text\n",
        "  for regex in regexList:\n",
        "    newText = re.sub(regex, '', str(newText))\n",
        "  return newText\n"
      ],
      "metadata": {
        "id": "-IhxNpi2AfsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN (Crawler)"
      ],
      "metadata": {
        "id": "rVOz67w3ArXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import string\n",
        "import re\n",
        "\n",
        "header = {\n",
        "  \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.82 Safari/537.36\"\n",
        "}\n",
        "\n",
        "url1 = \"https://edition.cnn.com/2022/11/02/politics/biden-speech-democracy-dc/index.html\"\n",
        "\n",
        "def scrapeCnn(url):\n",
        "  html_text = requests.get(url, headers=header).text\n",
        "  soup = BeautifulSoup(html_text, \"lxml\")\n",
        "  dic = {}\n",
        "  textContainer = soup.find(\"div\", class_=\"article__content\")\n",
        "  pArray = textContainer.find_all(\"p\")\n",
        "  raw_text = getText(pArray, ['<.*?>', '\\\\n'])\n",
        "  dic[\"raw text\"] = raw_text\n",
        "  #print(dic[\"raw text\"])\n",
        "  return dic\n",
        "\n",
        "def getText(text, regexList):\n",
        "  newText = text\n",
        "  for regex in regexList:\n",
        "    newText = re.sub(regex, '', str(newText))\n",
        "  return newText\n"
      ],
      "metadata": {
        "id": "bRsVn1YUA0h8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RealInfoSec (Crawler)"
      ],
      "metadata": {
        "id": "XpBn1ppVBDUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import string\n",
        "import re\n",
        "\n",
        "header = {\n",
        "  \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.82 Safari/537.36\"\n",
        "}\n",
        "\n",
        "url1 = \"https://www.realinfosec.net/cybersecurity-news/cosmiss-vulnerability-found-in-microsoft-azure-developer-tool/\"\n",
        "\n",
        "def scrapeRIS(url):\n",
        "  html_text = requests.get(url, headers=header).text\n",
        "  soup = BeautifulSoup(html_text, \"lxml\")\n",
        "  dic = {}\n",
        "  textContainer = soup.find(\"div\", class_=\"entry-content clear\")\n",
        "  pArray = textContainer.find_all(\"p\")\n",
        "  raw_text = getText(pArray, ['<.*?>'])\n",
        "  dic[\"raw text\"] = raw_text\n",
        "  #print(dic[\"raw text\"])\n",
        "  return dic\n",
        "\n",
        "def getText(text, regexList):\n",
        "  newText = text\n",
        "  for regex in regexList:\n",
        "    newText = re.sub(regex, '', str(newText))\n",
        "  return newText\n"
      ],
      "metadata": {
        "id": "PFoLHw4wBHkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TechCrunch (Crawler)"
      ],
      "metadata": {
        "id": "OZwCBGcbBVp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import string\n",
        "import re\n",
        "\n",
        "header = {\n",
        "  \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.82 Safari/537.36\"\n",
        "}\n",
        "\n",
        "url1 = \"https://techcrunch.com/2022/11/02/eric-schmidt-backs-former-google-execs-digital-family-office-platform-in-90-million-funding/\"\n",
        "\n",
        "def scrapeTechCrunch(url):\n",
        "  html_text = requests.get(url, headers=header).text\n",
        "  soup = BeautifulSoup(html_text, \"lxml\")\n",
        "  dic = {}\n",
        "  textContainer = soup.find(\"div\", class_=\"article-content\")\n",
        "  pArray = textContainer.find_all(\"p\")\n",
        "  raw_text = getText(pArray, ['<.*?>'])\n",
        "  dic[\"raw text\"] = raw_text\n",
        "  #print(dic[\"raw text\"])\n",
        "  return dic\n",
        "\n",
        "def getText(text, regexList):\n",
        "  newText = text\n",
        "  for regex in regexList:\n",
        "    newText = re.sub(regex, '', str(newText))\n",
        "  return newText\n"
      ],
      "metadata": {
        "id": "M5dEZz0qBrvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ITSecurityGuru (Search / Crawler) "
      ],
      "metadata": {
        "id": "x7CEa6leBwMW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crawler"
      ],
      "metadata": {
        "id": "XvhU3SFOB4Yn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import string\n",
        "import re\n",
        "\n",
        "header = {\n",
        "  \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.82 Safari/537.36\"\n",
        "}\n",
        "\n",
        "url1 = \"https://www.itsecurityguru.org/2022/12/09/dragos-announces-partnership-with-cisco/\"\n",
        "\n",
        "def scrapeITGuru(url):\n",
        "  html_text = requests.get(url, headers=header).text\n",
        "  soup = BeautifulSoup(html_text, \"lxml\")\n",
        "  dic = {}\n",
        "  textContainer = soup.find(\"div\", class_=\"content-inner \")\n",
        "  raw_text = \"\"\n",
        "  pArray = textContainer.find_all(\"p\")\n",
        "  for paragraph in pArray:\n",
        "    raw_text += paragraph.get_text()\n",
        "  #print(raw_text)\n",
        "  dic[\"raw text\"] = raw_text\n",
        "  return dic"
      ],
      "metadata": {
        "id": "0_udj5E_B563"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Search"
      ],
      "metadata": {
        "id": "O_qCu1N_CujO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search_dic = collections.defaultdict(list)\n",
        "date_dic = collections.defaultdict(list)\n",
        "\n",
        "def check_eop(html_text, max_year):#end of page\n",
        "  date_regex = 'class=\"fa fa-clock-o\"><\\/i>(.*, (\\d*))<\\/a><\\/div>'\n",
        "  date = re.search(date_regex, html_text)\n",
        "  if date is not None:\n",
        "    date_year = date.group(2)\n",
        "    if int(date_year) >= max_year:\n",
        "      return False, date_year\n",
        "    else:\n",
        "      return True, 0\n",
        "  return False, 0\n",
        "\n",
        "def getSearchResults():\n",
        "  \n",
        "  queries1 = ['fraud', 'hacker groups', 'government', 'corporation',\n",
        "       'darknet', 'cyber defense', 'hacking', 'security concepts',\n",
        "       'security products', 'network security', 'cyberwar', 'geopolitical',\n",
        "       'data breach', 'vulnerability', 'platform', 'cyber attack']\n",
        "\n",
        "  queries=['fraud']\n",
        "  for query in all_tags:\n",
        "    print(\"Searching: \" + query)\n",
        "\n",
        "    driver = webdriver.Chrome('chromedriver',options=chrome_options)\n",
        "    driver.maximize_window()\n",
        "    current_page = 1\n",
        "    end_of_page = False\n",
        "    url= 'https://www.itsecurityguru.org/page/' + str(current_page) + '/?s=' + query #get first page\n",
        "    driver.get(url)\n",
        "    time.sleep(5)\n",
        "\n",
        "    regex_last_page = '(<a class=\"page_number\").*\"(\\d*)\".*(\\s<a class=\"page_nav next\")'\n",
        "    last_page = re.search(regex_last_page, str(driver.page_source))#get last page\n",
        "\n",
        "    if last_page is not None:\n",
        "      last_page = last_page.group(2)\n",
        "      print(\"last_page: \" + str(last_page))\n",
        "      for page_no in range(1, int(last_page) + 1):#loop all pages\n",
        "        if not end_of_page:\n",
        "          print(\"current page: \" + str(page_no))\n",
        "          url= 'https://www.itsecurityguru.org/page/' + str(page_no) + '/?s=' + query\n",
        "          driver.get(url)\n",
        "\n",
        "          soup = BeautifulSoup(driver.page_source, 'lxml')\n",
        "          news_list = soup.find_all(\"article\", {\"class\": \"jeg_post jeg_pl_md_2 format-standard\"})\n",
        "\n",
        "          for new in news_list:\n",
        "            #print(len(news_list))\n",
        "            date_result = check_eop(str(new), 2017)\n",
        "            if not date_result[0]:#check date\n",
        "              href_regex = '<a href=.*(\"https.*\\/\")><div class'\n",
        "              web_url = re.search(href_regex, str(new))#get url\n",
        "              if web_url is not None:\n",
        "                web_url = web_url.group(1)\n",
        "                if query not in search_dic[web_url]:#check unique\n",
        "                  search_dic[web_url].append(query)\n",
        "                  date_dic[web_url].append(date_result[1])\n",
        "            # else:\n",
        "            #   print(\"END OF PAGE\")\n",
        "            #   end_of_page = True\n",
        "\n",
        "    driver.close()\n",
        "\n",
        "def getText(text, regexList):\n",
        "  newText = text\n",
        "  for regex in regexList:\n",
        "    newText = re.sub(regex, '', str(newText))\n",
        "  return newText"
      ],
      "metadata": {
        "id": "K5O0ZgTNCxP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "getSearchResults()"
      ],
      "metadata": {
        "id": "mpdjrPWEXx8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BleepingComputer (Crawler / Search)"
      ],
      "metadata": {
        "id": "JZI5gJa7ERmx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crawler (Not complete?)"
      ],
      "metadata": {
        "id": "JOoUBb7wErJe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "# from nltk.stem import PorterStemmer\n",
        "# porter=PorterStemmer()\n",
        "\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "englishStemmer=SnowballStemmer(\"english\")\n",
        "\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWGzs1PcQn6Y",
        "outputId": "47b8db5a-4d5e-44fd-986a-72d314a40b3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import string\n",
        "header = {\n",
        "  \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.82 Safari/537.36\"\n",
        "}\n",
        "\n",
        "def removeStopWords(sentence):\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  word_tokens = word_tokenize(sentence)\n",
        "  filtered_sentence = [w.lower() for w in word_tokens if not w.lower() in stop_words]\n",
        "\n",
        "  # filtered_sentence to single string\n",
        "  newSentence = \"\"\n",
        "\n",
        "  for word in filtered_sentence:\n",
        "    newSentence += word + \" \"\n",
        "\n",
        "  newSentence = newSentence.translate(newSentence.maketrans(\"\", \"\", string.punctuation + \"“”’0123456789\"))\n",
        "\n",
        "  return newSentence\n",
        "\n",
        "\n",
        "# tokenize the sentence\n",
        "def tokenize(sentence):\n",
        "  \n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(sentence)\n",
        "  sequences = tokenizer.texts_to_sequences(sentence)\n",
        "\n",
        "\n",
        "  word_index = tokenizer.word_index\n",
        "  bow = {}\n",
        "  for key in word_index:\n",
        "      bow[key] = sequences[0].count(word_index[key])\n",
        "\n",
        "  return bow\n",
        "\n",
        "def stemSentence(sentence):\n",
        "  token_words=word_tokenize(sentence)\n",
        "  stem_sentence=[]\n",
        "  for word in token_words:\n",
        "    w = (wordnet_lemmatizer.lemmatize(word, pos=\"n\"))\n",
        "    stem_sentence.append(englishStemmer.stem(w))\n",
        "    stem_sentence.append(\" \")\n",
        "  return \"\".join(stem_sentence)\n",
        "\n",
        "def mostFrequent(List):\n",
        "  counter = 0\n",
        "  num = List[0]\n",
        "    \n",
        "  for i in List:\n",
        "    curr_frequency = List.count(i)\n",
        "    if(curr_frequency> counter):\n",
        "      counter = curr_frequency\n",
        "      num = i\n",
        "\n",
        "  return num\n",
        "\n",
        "def mostFrequentList(List):\n",
        "  counter = 0\n",
        "  num = []\n",
        "    \n",
        "  for i in List:\n",
        "    curr_frequency = List.count(i)\n",
        "    if(curr_frequency> counter):\n",
        "      counter = curr_frequency\n",
        "      num = [i]\n",
        "    elif(curr_frequency == counter and i not in num):\n",
        "      num.append(i)\n",
        "\n",
        "  return num\n",
        "\n",
        "def scrapeAll(url):\n",
        "\n",
        "  newsList = []\n",
        "  html_text = requests.get(url, headers = header).text\n",
        "  soup = BeautifulSoup(html_text, \"lxml\")\n",
        "  allDivs = soup.find_all(\"a\") # find all \"a\" tags\n",
        "  # allDivs = soup.find_all(\"div\") # find all \"div\" tags\n",
        "\n",
        "  classListArr = []\n",
        "\n",
        "  for div in allDivs: \n",
        "    # if(div.attrs.get(\"class\") and len(div.text.strip()) > 0 and div.attrs.get(\"class\")[0].find(\"image\") == -1 and div.attrs.get(\"class\")[0].find(\"img\") == -1):\n",
        "    if(div.attrs.get(\"class\") and len(div.text.strip()) > 0):\n",
        "      classListArr.append(div.attrs.get(\"class\")[0])\n",
        "\n",
        "\n",
        "  # print(classListArr)\n",
        "  commonClass = mostFrequent(classListArr) # get the most common class name\n",
        "  # print(commonClass)\n",
        "  allNews = soup.find_all(\"a\", class_ = commonClass) # get all the \"a\" tags with most common class name\n",
        "  # allNews = soup.find_all(\"a\", class_ = commonClass) # get all the \"div\" tags with most common class name\n",
        "\n",
        "  # for new in allNews:\n",
        "  #   print(new)\n",
        "\n",
        "  # print(allTitles)\n",
        "\n",
        "\n",
        "  for news in allNews:\n",
        "    dic = {}\n",
        "    newsText = news.text.strip()\n",
        "    newsLink = news.get(\"href\")\n",
        "    if(newsLink.find(\"http\") == -1): # if link is broken\n",
        "      newUrl = url\n",
        "      if(len(url.split(\"/\")) > 3):\n",
        "        # newUrl = url[:url.rfind(\"/\")]\n",
        "        newUrl = url.split(\"/\")[0] + \"//\" + url.split(\"/\")[2]\n",
        "      newsLink = newUrl + newsLink\n",
        "\n",
        "    # dic[\"text\"] = newsText\n",
        "    dic[\"header\"] = newsText\n",
        "    dic[\"link\"] = newsLink\n",
        "    newsList.append(dic)\n",
        "    # print(f'text: {newsText} \\nlink: {newsLink}')\n",
        "    # print(f'text: {newsText}')\n",
        "    # print(f'link: {newsLink}\\n')\n",
        "\n",
        "  return newsList\n",
        "\n",
        "def take_second(elem):\n",
        "    return elem[1]\n",
        "\n",
        "def sortWordOccurences(sentence):\n",
        "  x = removeStopWords(sentence)\n",
        "  x = x.translate(x.maketrans(\"\", \"\", string.punctuation + \"“”’0123456789\"))\n",
        "  x= tokenize([x])\n",
        "  x = sorted(x.items(), reverse = True, key = take_second)\n",
        "\n",
        "  return x"
      ],
      "metadata": {
        "id": "V5rE4xQJPY-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def take_second(elem):\n",
        "    return elem[1]\n",
        "\n",
        "def sortWordOccurences(sentence):\n",
        "  x = removeStopWords(sentence)\n",
        "  x = x.translate(x.maketrans(\"\", \"\", string.punctuation + \"“”’0123456789\"))\n",
        "  x= tokenize([x])\n",
        "  x = sorted(x.items(), reverse = True, key = take_second)\n",
        "\n",
        "  return x"
      ],
      "metadata": {
        "id": "dsxwH7CDPvMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categoryMap = {\"security\": 1, \"non-security\": 0}\n",
        "\n",
        "# df = pd.DataFrame(columns = [\"title\", \"first two paragraph\", \"url\", \"date\", \"stemmed text\", \"category\"])\n",
        "mainDf = pd.DataFrame()"
      ],
      "metadata": {
        "id": "-XOMhlivE6QY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BLEEPINGCOMPUTER.COM\n",
        "\n",
        "def tokenizeBleepingComputer(url):\n",
        "  data = requests.get(url, headers = header).text\n",
        "  new_soup = BeautifulSoup(data, \"lxml\")\n",
        "  body = new_soup.find_all(\"div\", class_ = \"articleBody\")\n",
        "\n",
        "  newsText = \"\"\n",
        "\n",
        "\n",
        "  for txt in body:\n",
        "    newsText += txt.text.strip();\n",
        "\n",
        "  newsText = newsText.translate(newsText.maketrans(\"\", \"\", string.punctuation + \"“”0123456789\"))\n",
        "\n",
        "  sentence = removeStopWords(newsText)\n",
        "  sentence = stemSentence(sentence)\n",
        "  return sentence\n",
        "  # return sentence.split(\" \")\n",
        "  # return tokenize([sentence])\n",
        "\n",
        "def scrapeBleepingComputer(page):\n",
        "  html_text = requests.get('https://bleepingcomputer.com/' + page, headers = header).text\n",
        "  soup = BeautifulSoup(html_text, \"lxml\")\n",
        "  # print(soup)\n",
        "  allNews = soup.find_all(\"div\", class_ = \"bc_latest_news_text\")\n",
        "  \n",
        "  df = pd.DataFrame()\n",
        "\n",
        "  for news in allNews:\n",
        "    dic = {}\n",
        "    date = news.ul.contents[2].text\n",
        "    category = news.div.a.text\n",
        "    title = news.h4.a\n",
        "    firstParagraph = news.p.text\n",
        "    title_text = title.text\n",
        "    title_link = title[\"href\"]\n",
        "    dic[\"title\"] = title_text\n",
        "    dic[\"first paragraph\"] = firstParagraph\n",
        "    dic[\"url\"] = title_link\n",
        "    dic[\"date\"] = date\n",
        "    if(category == \"Security\"):\n",
        "      dic[\"category\"] = \"security\"\n",
        "    else:\n",
        "      dic[\"category\"] = \"non-security\"\n",
        "    dic[\"stemmed text\"] = tokenizeBleepingComputer(title_link)\n",
        "    df = df.append(dic, ignore_index = True)\n",
        "  return df"
      ],
      "metadata": {
        "id": "5wgSbY7qEssg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(100):\n",
        "  if(i == 0):\n",
        "    df = scrapeBleepingComputer(\"\")\n",
        "  else:\n",
        "    df = scrapeBleepingComputer(\"page/\" + str(i + 1) + \"/\")\n",
        "  mainDf = mainDf.append(df, ignore_index= True)"
      ],
      "metadata": {
        "id": "VOlMobVNETk5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ba6ea9cd-ab1d-45fa-de8d-5d9866d259b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4.zip/omw-1.4/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-92da3f21bbca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrapeBleepingComputer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrapeBleepingComputer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"page/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-f6037b70963d>\u001b[0m in \u001b[0;36mscrapeBleepingComputer\u001b[0;34m(page)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m       \u001b[0mdic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"category\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"non-security\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mdic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stemmed text\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizeBleepingComputer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle_link\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-f6037b70963d>\u001b[0m in \u001b[0;36mtokenizeBleepingComputer\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremoveStopWords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewsText\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m   \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstemSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0;31m# return sentence.split(\" \")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-8eb70852faab>\u001b[0m in \u001b[0;36mstemSentence\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     43\u001b[0m   \u001b[0mstem_sentence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoken_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwordnet_lemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mstem_sentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menglishStemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mstem_sentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/stem/wordnet.py\u001b[0m in \u001b[0;36mlemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mlemma\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mword\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \"\"\"\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mlemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__reader_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m# This is where the magic happens!  Transform ourselves into\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, omw_reader)\u001b[0m\n\u001b[1;32m   1174\u001b[0m             )\n\u001b[1;32m   1175\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprovenances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0momw_prov\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0;31m# A cache to store the wordnet data of multiple languages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36momw_prov\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0mprovdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0mprovdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eng\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m         \u001b[0mfileids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_omw_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfileid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m             \u001b[0mprov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlangfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{self.__name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Search"
      ],
      "metadata": {
        "id": "cx4feUGlWxIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "driver = webdriver.Chrome('chromedriver',options=chrome_options)\n",
        "import collections\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.wait import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC"
      ],
      "metadata": {
        "id": "cotLidzTWzbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import string\n",
        "import re\n",
        "import json\n",
        "import collections"
      ],
      "metadata": {
        "id": "uWooDMdjW0Fu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_dic = collections.defaultdict(list)\n",
        "html_list = []\n",
        "len(all_tags)"
      ],
      "metadata": {
        "id": "YOWSOhoDW3XX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getSearchResults(tag_list):\n",
        "  queries1 = ['fraud', 'hacker groups', 'government', 'corporation',\n",
        "       'darknet', 'cyber defense', 'hacking', 'security concepts',\n",
        "       'security products', 'network security', 'cyberwar', 'geopolitical',\n",
        "       'data breach', 'vulnerability', 'platform', 'cyber attack']\n",
        "\n",
        "  queries=['fraud', 'hacking']\n",
        "  \n",
        "  driver = webdriver.Chrome('chromedriver',options=chrome_options)\n",
        "  driver.maximize_window()\n",
        "\n",
        "  for tag in tag_list:\n",
        "    print(len(search_dic))\n",
        "    print(search_dic) \n",
        "    print(\"Searching: \" + tag)\n",
        "    \n",
        "    url = 'https://www.bleepingcomputer.com/search/?cx=partner-pub-0920899300397823%3A3529943228&cof=FORID%3A10&ie=UTF-8&q='+ tag\n",
        "    driver.get(url)\n",
        "    time.sleep(3)\n",
        "\n",
        "    current_page = 1\n",
        "    for i in range(1, 11):\n",
        "      print(\"Page:\", current_page)\n",
        "      soup = BeautifulSoup(driver.page_source, 'lxml')\n",
        "      all_news = soup.find_all('div', {\"class\": \"gsc-webResult gsc-result\"})\n",
        "      for new in all_news:\n",
        "        span = new.find('div', {\"class\": \"gs-bidi-start-align gs-visibleUrl gs-visibleUrl-breadcrumb\"})#for span News\n",
        "        if check_span(span.text):\n",
        "          date = new.find('div', {\"class\": \"gs-bidi-start-align gs-snippet\"})\n",
        "          if check_date(date.text, 2017):\n",
        "            href = new.find('a', {\"class\": \"gs-title\"})\n",
        "            if href['href'] is not None:\n",
        "              print(\"url\", href['href'])\n",
        "              web_url = href['href']\n",
        "              if tag not in search_dic[web_url]:\n",
        "                search_dic[web_url].append(tag)\n",
        "                \n",
        "      time.sleep(1)\n",
        "      current_page = current_page + 1\n",
        "      if current_page <= 10:\n",
        "        #button = driver.find_element(By.CSS_SELECTOR, 'div.gsc-cursor-page:nth-child(' + str(current_page) + ')')\n",
        "        button = WebDriverWait(driver, 120).until(EC.presence_of_element_located((By.CSS_SELECTOR, 'div.gsc-cursor-page:nth-child(' + str(current_page) + ')')))\n",
        "        if button is not None:\n",
        "          driver.execute_script(\"arguments[0].click();\", button)\n",
        "          time.sleep(3)\n",
        "\n",
        "\n",
        "def check_date(html_text, min_year):\n",
        "  if(html_text):\n",
        "    recent =  re.search('days ago', html_text[0:20])\n",
        "    if recent is not None:\n",
        "      return True\n",
        "    else:\n",
        "      date = re.search(r\"\\d, (\\b\\d{4}\\b)\", html_text[0:20])\n",
        "      if date is not None:\n",
        "        date_year = date.group(1)\n",
        "        if int(date_year) >= min_year:\n",
        "          return True\n",
        "  return False\n",
        "\n",
        "def check_span(html_text):\n",
        "  if(html_text):\n",
        "    element = re.search('.* › (.*?) › .*', html_text)\n",
        "    if element is not None:\n",
        "      element_value = element.group(1)\n",
        "      if element_value == \"News\":\n",
        "        return True\n",
        "  return False\n",
        "\n",
        "def parsehtml(html_text):\n",
        "  soup = BeautifulSoup(html_text, 'lxml')\n",
        "  html_list.append(html_text)\n",
        "  all_news = soup.find_all('div', {\"class\": \"gsc-webResult gsc-result\"})#get all news in the page\n",
        "  for new in all_news:\n",
        "    span = new.find('div', {\"class\": \"gs-bidi-start-align gs-visibleUrl gs-visibleUrl-breadcrumb\"})#check if post type is news\n",
        "    if span is not None: \n",
        "      if check_span(span.text):\n",
        "        date = new.find('div', {\"class\": \"gs-bidi-start-align gs-snippet\"})\n",
        "        if date is not None:\n",
        "          if check_date(date.text, 2017):\n",
        "            href = new.find('a', {\"class\": \"gs-title\"})\n",
        "            if href['href'] is not None:\n",
        "              print(\"url\", href['href'])\n",
        "              web_url = href['href']\n",
        "              if tag not in search_dic[web_url]:\n",
        "                search_dic[web_url].append(tag)"
      ],
      "metadata": {
        "id": "1HtBcLuNW7Dz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "getSearchResults(all_tags)"
      ],
      "metadata": {
        "id": "kyk4PeTQW8c4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SCMagazine (Search / Crawler)"
      ],
      "metadata": {
        "id": "w9GcssXiXMW8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Search"
      ],
      "metadata": {
        "id": "SznfR9sAXRQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')"
      ],
      "metadata": {
        "id": "XP25-_BkXSrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_dic = collections.defaultdict(list)\n",
        "date_dic = collections.defaultdict(list)\n",
        "all_dic = collections.defaultdict(list)\n",
        "\n",
        "def check_date(min_year, html_text):\n",
        "  date_regex = 'itemprop=\"datePublished\">.*, (.*?)<\\/time>'\n",
        "  #date_regex = 'datetime=\\\"(.*?)-.*\\\" itemprop'\n",
        "  date_year = re.search(date_regex, html_text)\n",
        "  #print(html_text)\n",
        "  if date_year is not None:\n",
        "    date_year = int(date_year.group(1))\n",
        "    if date_year >= min_year:\n",
        "      return True, date_year\n",
        "  else:\n",
        "    print(\"date zort\")\n",
        "  return False, 0\n",
        "\n",
        "def check_article_type(html_text):\n",
        "  articleType_regex = 'searchTeaserCard_label__Szqey\">(.*?)<\\/div>'\n",
        "  article_type = re.search(articleType_regex, html_text)\n",
        "  if article_type is not None:\n",
        "    article_type = article_type.group(1)\n",
        "    if article_type in [\"brandview\", \"news\", \"brief\", \"analysis\", \"resource\", \"feature\"]:\n",
        "      print(article_type)\n",
        "      return True, article_type\n",
        "  return False, \"\"\n",
        "#\\/(.*?)\\/.*\n",
        "\n",
        "def check_article_type2(web_url):\n",
        "  articleType_regex = '\\/(.*?)\\/.*'\n",
        "  article_type = re.search(articleType_regex, web_url)\n",
        "  if article_type is not None:\n",
        "    article_type = article_type.group(1)\n",
        "    if article_type in [\"brandview\", \"news\", \"brief\", \"analysis\", \"resource\", \"feature\"]:\n",
        "      return True, article_type\n",
        "  return False, \"\"\n",
        "\n",
        "def getSearchResults():\n",
        "  \n",
        "  queries1 = ['fraud', 'hacker groups', 'government', 'corporation',\n",
        "       'darknet', 'cyber defense', 'hacking', 'security concepts',\n",
        "       'security products', 'network security', 'cyberwar', 'geopolitical',\n",
        "       'data breach', 'vulnerability', 'platform', 'cyber attack']\n",
        "\n",
        "  queries=['darknet']\n",
        "  for query in queries:\n",
        "    print(\"Searching: \" + query)\n",
        "\n",
        "    driver = webdriver.Chrome('chromedriver',options=chrome_options)\n",
        "    driver.maximize_window()\n",
        "\n",
        "    url = 'https://www.scmagazine.com/search?q=' + query\n",
        "    driver.get(url)\n",
        "    time.sleep(5)  # Allow 2 seconds for the web page to open\n",
        "    scroll_pause_time = 3 # You can set your own pause time. My laptop is a bit slow so I use 1 sec\n",
        "    screen_height = driver.execute_script(\"return window.screen.height;\")   # get the screen height of the web\n",
        "    i = 1\n",
        "\n",
        "    while True:\n",
        "        # scroll one screen height each time\n",
        "        driver.execute_script(\"window.scrollTo(0, {screen_height}*{i});\".format(screen_height=screen_height, i=i))  \n",
        "        i += 1\n",
        "        time.sleep(scroll_pause_time)\n",
        "        # update scroll height each time after scrolled, as the scroll height can change after we scrolled the page\n",
        "        scroll_height = driver.execute_script(\"return document.body.scrollHeight;\")  \n",
        "        # Break the loop when the height we need to scroll to is larger than the total scroll height\n",
        "        if (screen_height) * i > scroll_height:\n",
        "            break\n",
        "\n",
        "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
        "    news_list = soup.find_all(\"div\", {\"class\": \"teaser-list\"})\n",
        "    for news_group in news_list:\n",
        "      for new in news_group:\n",
        "        href_regex = 'class=\"font-sans\".* href=\"(.*?)\">'\n",
        "        web_url = re.search(href_regex, str(new))#get url\n",
        "        if web_url is not None:\n",
        "          web_url = web_url.group(1)\n",
        "\n",
        "          if query not in all_dic['https://www.scmagazine.com' + web_url]:\n",
        "            all_dic['https://www.scmagazine.com' + web_url].append(query)\n",
        "\n",
        "          article_type_result = check_article_type2(web_url)\n",
        "          if article_type_result[0]:\n",
        "            date_result = check_date(2017, str(new))\n",
        "            if date_result[0]:\n",
        "              if query not in search_dic['https://www.scmagazine.com' + web_url]:#check unique tag\n",
        "                search_dic['https://www.scmagazine.com' + web_url].append(query)\n",
        "                date_dic['https://www.scmagazine.com' + web_url].append(date_result[1])\n",
        "                print('https://www.scmagazine.com' + web_url)\n",
        "          \n",
        "    driver.close()"
      ],
      "metadata": {
        "id": "2mNjeSguXWOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "getSearchResults()"
      ],
      "metadata": {
        "id": "vtt06PLHXW0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(search_dic))\n",
        "print(len(all_dic))\n",
        "print((search_dic))\n",
        "print((all_dic))"
      ],
      "metadata": {
        "id": "EpvxUcQ9Xda4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}